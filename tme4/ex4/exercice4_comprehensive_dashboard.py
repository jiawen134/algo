#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
TME4 Exercice 4 - Comprehensive Network Analysis Dashboard
Tableau de bord complet intÃ©grant les analyses des Exercices 1, 2 et 3
Analyse multi-temporelle du rÃ©seau Rollernet avec visualisations avancÃ©es
"""

import gzip
import csv
import heapq
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
from collections import defaultdict, Counter
import networkx as nx
import random
from datetime import datetime
import os

class RollernetComprehensiveAnalyzer:
    """Analyseur complet pour le rÃ©seau Rollernet intÃ©grant toutes les mÃ©thodes TME4"""
    
    def __init__(self, data_file="tme4/ex1/rollernet.dyn.gz"):
        self.data_file = data_file
        self.temporal_edges = []
        self.time_windows = {}
        self.static_graphs = {}
        self.analysis_results = {}
        
        # Configuration des fenÃªtres temporelles (en secondes)
        self.window_configs = {
            'window1': (600, 1200),    # 10-20 minutes
            'window2': (1200, 1800),   # 20-30 minutes  
            'window3': (1800, 2400),   # 30-40 minutes
        }
        
        print("ğŸš€ TME4 Exercice 4 - Dashboard Complet Rollernet")
        print("=" * 60)
    
    def load_temporal_data(self):
        """Charger toutes les donnÃ©es temporelles"""
        print("ğŸ”„ Chargement des donnÃ©es temporelles...")
        
        with gzip.open(self.data_file, 'rt') as f:
            for line in f:
                line = line.strip()
                if not line or line.startswith('#'):
                    continue
                    
                parts = line.split()
                if len(parts) >= 3:
                    try:
                        node1, node2, timestamp = int(parts[0]), int(parts[1]), int(parts[2])
                        self.temporal_edges.append((timestamp, node1, node2))
                    except ValueError:
                        continue
        
        self.temporal_edges.sort()
        print(f"âœ… {len(self.temporal_edges)} arÃªtes temporelles chargÃ©es")
        
        # Organiser par fenÃªtres temporelles
        for window_name, (start, end) in self.window_configs.items():
            window_edges = [(t, n1, n2) for t, n1, n2 in self.temporal_edges if start <= t <= end]
            self.time_windows[window_name] = window_edges
            print(f"   {window_name}: {len(window_edges)} arÃªtes ({start}-{end}s)")
    
    def build_static_graphs(self):
        """Construire les graphes statiques pour chaque fenÃªtre"""
        print("\nğŸ”„ Construction des graphes statiques...")
        
        for window_name, edges in self.time_windows.items():
            G = nx.Graph()
            
            # Ajouter toutes les arÃªtes (agrÃ©gation temporelle)
            for timestamp, node1, node2 in edges:
                G.add_edge(node1, node2)
            
            self.static_graphs[window_name] = G
            print(f"   {window_name}: {G.number_of_nodes()} nÅ“uds, {G.number_of_edges()} arÃªtes")
    
    def analyze_basic_metrics(self):
        """Analyse 1: MÃ©triques de base (comme Exercice 1)"""
        print("\nğŸ”„ Analyse des mÃ©triques de base...")
        
        for window_name, G in self.static_graphs.items():
            metrics = {
                'nodes': G.number_of_nodes(),
                'edges': G.number_of_edges(),
                'density': nx.density(G),
                'connected_components': nx.number_connected_components(G),
                'largest_component_size': len(max(nx.connected_components(G), key=len)) if G.nodes() else 0,
            }
            
            # MÃ©triques de degrÃ©
            degrees = dict(G.degree())
            if degrees:
                metrics.update({
                    'avg_degree': np.mean(list(degrees.values())),
                    'max_degree': max(degrees.values()),
                    'min_degree': min(degrees.values()),
                    'degree_std': np.std(list(degrees.values()))
                })
            
            # Clustering coefficient
            if G.nodes():
                metrics['avg_clustering'] = nx.average_clustering(G)
            
            self.analysis_results[f'{window_name}_basic'] = metrics
            
    def analyze_centralities(self):
        """Analyse 2: CentralitÃ©s (comme Exercice 2)"""
        print("\nğŸ”„ Calcul des centralitÃ©s...")
        
        for window_name, G in self.static_graphs.items():
            if not G.nodes():
                continue
                
            print(f"   CentralitÃ©s pour {window_name}...")
            
            # Calculer les centralitÃ©s
            try:
                closeness = nx.closeness_centrality(G)
                betweenness = nx.betweenness_centrality(G)
                degree_centrality = nx.degree_centrality(G)
                eigenvector = nx.eigenvector_centrality(G, max_iter=1000)
            except:
                # Fallback pour graphes dÃ©connectÃ©s
                closeness = {node: 0 for node in G.nodes()}
                betweenness = {node: 0 for node in G.nodes()}
                degree_centrality = nx.degree_centrality(G)
                eigenvector = {node: 0 for node in G.nodes()}
            
            # Normaliser en pourcentages
            max_close = max(closeness.values()) if closeness.values() else 1
            max_between = max(betweenness.values()) if betweenness.values() else 1
            max_eigen = max(eigenvector.values()) if eigenvector.values() else 1
            
            centralities = {}
            for node in G.nodes():
                centralities[node] = {
                    'degree': G.degree(node),
                    'degree_centrality': degree_centrality[node] * 100,
                    'closeness': (closeness[node] / max_close * 100) if max_close > 0 else 0,
                    'betweenness': (betweenness[node] / max_between * 100) if max_between > 0 else 0,
                    'eigenvector': (eigenvector[node] / max_eigen * 100) if max_eigen > 0 else 0
                }
            
            self.analysis_results[f'{window_name}_centralities'] = centralities
    
    def analyze_temporal_paths(self, max_time_window=1800):
        """Analyse 3: Chemins temporels (comme Exercice 3)"""
        print("\nğŸ”„ Analyse des chemins temporels...")
        
        for window_name, edges in self.time_windows.items():
            print(f"   Chemins temporels pour {window_name}...")
            
            # Construire la structure temporelle
            node_timeline = defaultdict(list)
            nodes = set()
            
            for timestamp, node1, node2 in edges:
                nodes.add(node1)
                nodes.add(node2)
                node_timeline[node1].append((timestamp, node2))
                node_timeline[node2].append((timestamp, node1))
            
            # Trier les timelines
            for node in node_timeline:
                node_timeline[node].sort()
            
            # Calculer les centralitÃ©s temporelles (version Ã©chantillonnÃ©e)
            temporal_closeness = {}
            sample_nodes = random.sample(list(nodes), min(20, len(nodes)))
            
            for source in sample_nodes:
                total_time = 0
                reachable_count = 0
                start_time = min(timestamp for timestamp, _, _ in edges) if edges else 0
                
                # Tester accessibilitÃ© vers d'autres nÅ“uds
                target_sample = random.sample(list(nodes), min(10, len(nodes)))
                
                for target in target_sample:
                    if source != target:
                        # Dijkstra temporel simplifiÃ©
                        pq = [(start_time, source)]
                        visited = {source: start_time}
                        
                        found = False
                        while pq and not found:
                            current_time, current_node = heapq.heappop(pq)
                            
                            if current_time - start_time > max_time_window:
                                break
                                
                            for edge_time, neighbor in node_timeline[current_node]:
                                if edge_time >= current_time:
                                    if neighbor == target:
                                        travel_time = edge_time - start_time
                                        total_time += travel_time
                                        reachable_count += 1
                                        found = True
                                        break
                                    
                                    if neighbor not in visited or visited[neighbor] > edge_time:
                                        visited[neighbor] = edge_time
                                        heapq.heappush(pq, (edge_time, neighbor))
                
                if reachable_count > 0:
                    temporal_closeness[source] = (max_time_window / (total_time / reachable_count)) * 100
                else:
                    temporal_closeness[source] = 0
            
            self.analysis_results[f'{window_name}_temporal'] = temporal_closeness
    
    def create_comprehensive_dashboard(self):
        """CrÃ©er le tableau de bord complet"""
        print("\nğŸ“Š GÃ©nÃ©ration du tableau de bord complet...")
        
        # Configuration de la figure
        fig = plt.figure(figsize=(20, 16))
        gs = gridspec.GridSpec(4, 4, hspace=0.3, wspace=0.3)
        
        # 1. Ã‰volution des mÃ©triques de base
        ax1 = fig.add_subplot(gs[0, :2])
        self._plot_network_evolution(ax1)
        
        # 2. Comparison centralitÃ©s statiques
        ax2 = fig.add_subplot(gs[0, 2:])
        self._plot_centrality_comparison(ax2)
        
        # 3. Distribution des degrÃ©s par fenÃªtre
        ax3 = fig.add_subplot(gs[1, 0])
        self._plot_degree_distribution(ax3, 'window1')
        
        ax4 = fig.add_subplot(gs[1, 1])
        self._plot_degree_distribution(ax4, 'window2')
        
        ax5 = fig.add_subplot(gs[1, 2])
        self._plot_degree_distribution(ax5, 'window3')
        
        # 4. Heatmap centralitÃ©s
        ax6 = fig.add_subplot(gs[1, 3])
        self._plot_centrality_heatmap(ax6)
        
        # 5. Ã‰volution temporelle vs statique
        ax7 = fig.add_subplot(gs[2, :2])
        self._plot_temporal_vs_static(ax7)
        
        # 6. Top nÅ“uds par fenÃªtre
        ax8 = fig.add_subplot(gs[2, 2:])
        self._plot_top_nodes_evolution(ax8)
        
        # 7. MÃ©triques de connectivitÃ©
        ax9 = fig.add_subplot(gs[3, :2])
        self._plot_connectivity_metrics(ax9)
        
        # 8. RÃ©sumÃ© statistique
        ax10 = fig.add_subplot(gs[3, 2:])
        self._plot_statistical_summary(ax10)
        
        plt.suptitle('TME4 Ex4 - Dashboard Complet Rollernet\nAnalyse Multi-Temporelle et Comparative', 
                     fontsize=16, fontweight='bold', y=0.98)
        
        # Sauvegarder
        output_file = "tme4/ex4/rollernet_comprehensive_dashboard.png"
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"âœ… Dashboard sauvegardÃ©: {output_file}")
        return output_file
    
    def _plot_network_evolution(self, ax):
        """Graphique 1: Ã‰volution des mÃ©triques rÃ©seau"""
        windows = list(self.window_configs.keys())
        
        metrics_to_plot = ['nodes', 'edges', 'density']
        colors = ['blue', 'green', 'red']
        
        x_pos = np.arange(len(windows))
        width = 0.25
        
        for i, metric in enumerate(metrics_to_plot):
            values = []
            for window in windows:
                basic_data = self.analysis_results.get(f'{window}_basic', {})
                if metric == 'density':
                    values.append(basic_data.get(metric, 0) * 100)  # Pourcentage
                else:
                    values.append(basic_data.get(metric, 0))
            
            offset = (i - 1) * width
            bars = ax.bar(x_pos + offset, values, width, label=metric.title(), 
                         color=colors[i], alpha=0.7)
            
            # Ajouter les valeurs sur les barres
            for bar, value in zip(bars, values):
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height + height*0.01,
                       f'{value:.1f}', ha='center', va='bottom', fontsize=8)
        
        ax.set_xlabel('FenÃªtres Temporelles')
        ax.set_ylabel('Valeurs')
        ax.set_title('Ã‰volution des MÃ©triques RÃ©seau', fontweight='bold')
        ax.set_xticks(x_pos)
        ax.set_xticklabels(['10-20min', '20-30min', '30-40min'])
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    def _plot_centrality_comparison(self, ax):
        """Graphique 2: Comparaison centralitÃ©s"""
        window = 'window2'  # Utiliser la fenÃªtre du milieu
        centralities = self.analysis_results.get(f'{window}_centralities', {})
        
        if not centralities:
            ax.text(0.5, 0.5, 'DonnÃ©es indisponibles', ha='center', va='center')
            return
        
        nodes = list(centralities.keys())
        degrees = [centralities[node]['degree'] for node in nodes]
        closeness = [centralities[node]['closeness'] for node in nodes]
        betweenness = [centralities[node]['betweenness'] for node in nodes]
        
        # Scatter plot comme Ex2
        ax.scatter(degrees, closeness, color='blue', marker='s', s=50, 
                  alpha=0.7, label='Closeness', edgecolors='navy')
        ax.scatter(degrees, betweenness, color='orange', marker='D', s=50,
                  alpha=0.7, label='Betweenness', edgecolors='darkorange')
        
        ax.set_xlabel('DegrÃ©')
        ax.set_ylabel('CentralitÃ© (%)')
        ax.set_title(f'CentralitÃ©s vs DegrÃ© (20-30min)', fontweight='bold')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # CorrÃ©lations
        if len(degrees) > 1:
            corr_close = np.corrcoef(degrees, closeness)[0, 1]
            corr_between = np.corrcoef(degrees, betweenness)[0, 1]
            ax.text(0.05, 0.95, f'r(deg,close)={corr_close:.3f}\nr(deg,between)={corr_between:.3f}',
                   transform=ax.transAxes, va='top', fontsize=9,
                   bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
    
    def _plot_degree_distribution(self, ax, window_name):
        """Graphique 3-5: Distribution des degrÃ©s"""
        centralities = self.analysis_results.get(f'{window_name}_centralities', {})
        
        if not centralities:
            ax.text(0.5, 0.5, 'DonnÃ©es\nindisponibles', ha='center', va='center')
            return
        
        degrees = [centralities[node]['degree'] for node in centralities]
        
        ax.hist(degrees, bins=15, color='skyblue', alpha=0.7, edgecolor='black')
        ax.set_xlabel('DegrÃ©')
        ax.set_ylabel('Nombre de NÅ“uds')
        
        window_labels = {'window1': '10-20min', 'window2': '20-30min', 'window3': '30-40min'}
        ax.set_title(f'Distribution DegrÃ©s\n{window_labels[window_name]}', fontweight='bold')
        ax.grid(True, alpha=0.3)
        
        # Statistiques
        if degrees:
            mean_deg = np.mean(degrees)
            ax.axvline(mean_deg, color='red', linestyle='--', alpha=0.8, label=f'Moyenne: {mean_deg:.1f}')
            ax.legend()
    
    def _plot_centrality_heatmap(self, ax):
        """Graphique 6: Heatmap des centralitÃ©s"""
        # Collecter les top 10 nÅ“uds par centralitÃ© closeness
        all_nodes = set()
        for window in self.window_configs.keys():
            centralities = self.analysis_results.get(f'{window}_centralities', {})
            if centralities:
                # Top 5 nÅ“uds par closeness
                top_nodes = sorted(centralities.items(), 
                                 key=lambda x: x[1]['closeness'], reverse=True)[:5]
                all_nodes.update([node for node, _ in top_nodes])
        
        if not all_nodes:
            ax.text(0.5, 0.5, 'DonnÃ©es\nindisponibles', ha='center', va='center')
            return
        
        # CrÃ©er la matrice heatmap
        windows = list(self.window_configs.keys())
        nodes_list = sorted(list(all_nodes))
        
        heatmap_data = []
        for node in nodes_list:
            row = []
            for window in windows:
                centralities = self.analysis_results.get(f'{window}_centralities', {})
                closeness = centralities.get(node, {}).get('closeness', 0)
                row.append(closeness)
            heatmap_data.append(row)
        
        im = ax.imshow(heatmap_data, cmap='YlOrRd', aspect='auto')
        
        ax.set_xticks(range(len(windows)))
        ax.set_xticklabels(['10-20min', '20-30min', '30-40min'])
        ax.set_yticks(range(len(nodes_list)))
        ax.set_yticklabels([f'N{node}' for node in nodes_list])
        ax.set_title('Heatmap Closeness\nTop NÅ“uds', fontweight='bold')
        
        # Colorbar
        plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)
    
    def _plot_temporal_vs_static(self, ax):
        """Graphique 7: Comparaison temporel vs statique"""
        window = 'window2'
        static_data = self.analysis_results.get(f'{window}_centralities', {})
        temporal_data = self.analysis_results.get(f'{window}_temporal', {})
        
        if not static_data or not temporal_data:
            ax.text(0.5, 0.5, 'DonnÃ©es temporelles\nindisponibles', ha='center', va='center')
            return
        
        # Trouver les nÅ“uds communs
        common_nodes = set(static_data.keys()) & set(temporal_data.keys())
        
        if not common_nodes:
            ax.text(0.5, 0.5, 'Pas de nÅ“uds\ncommuns', ha='center', va='center')
            return
        
        static_values = [static_data[node]['closeness'] for node in common_nodes]
        temporal_values = [temporal_data[node] for node in common_nodes]
        
        ax.scatter(static_values, temporal_values, alpha=0.7, s=50, color='purple')
        
        # Ligne de rÃ©fÃ©rence
        max_val = max(max(static_values) if static_values else [0], 
                     max(temporal_values) if temporal_values else [0])
        if max_val > 0:
            ax.plot([0, max_val], [0, max_val], 'r--', alpha=0.5, label='RÃ©fÃ©rence')
        
        ax.set_xlabel('Closeness Statique (%)')
        ax.set_ylabel('Closeness Temporelle (%)')
        ax.set_title('Comparaison Statique vs Temporel', fontweight='bold')
        ax.grid(True, alpha=0.3)
        ax.legend()
        
        # CorrÃ©lation
        if len(static_values) > 1 and len(temporal_values) > 1:
            corr = np.corrcoef(static_values, temporal_values)[0, 1]
            ax.text(0.05, 0.95, f'CorrÃ©lation: r={corr:.3f}',
                   transform=ax.transAxes, va='top',
                   bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
    
    def _plot_top_nodes_evolution(self, ax):
        """Graphique 8: Ã‰volution des top nÅ“uds"""
        windows = list(self.window_configs.keys())
        window_labels = ['10-20min', '20-30min', '30-40min']
        
        # Collecter les top 3 nÅ“uds par fenÃªtre
        top_nodes_data = {}
        
        for i, window in enumerate(windows):
            centralities = self.analysis_results.get(f'{window}_centralities', {})
            if centralities:
                top_3 = sorted(centralities.items(), 
                              key=lambda x: x[1]['closeness'], reverse=True)[:3]
                for rank, (node, data) in enumerate(top_3):
                    if node not in top_nodes_data:
                        top_nodes_data[node] = [0, 0, 0]
                    top_nodes_data[node][i] = data['closeness']
        
        if not top_nodes_data:
            ax.text(0.5, 0.5, 'DonnÃ©es\nindisponibles', ha='center', va='center')
            return
        
        # Tracer l'Ã©volution
        x_pos = np.arange(len(windows))
        colors = plt.cm.Set3(np.linspace(0, 1, len(top_nodes_data)))
        
        for (node, values), color in zip(top_nodes_data.items(), colors):
            ax.plot(x_pos, values, marker='o', label=f'NÅ“ud {node}', 
                   color=color, linewidth=2, markersize=6)
        
        ax.set_xlabel('FenÃªtres Temporelles')
        ax.set_ylabel('Closeness (%)')
        ax.set_title('Ã‰volution Top NÅ“uds', fontweight='bold')
        ax.set_xticks(x_pos)
        ax.set_xticklabels(window_labels)
        ax.grid(True, alpha=0.3)
        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    
    def _plot_connectivity_metrics(self, ax):
        """Graphique 9: MÃ©triques de connectivitÃ©"""
        windows = list(self.window_configs.keys())
        window_labels = ['10-20min', '20-30min', '30-40min']
        
        # MÃ©triques Ã  tracer
        density_values = []
        clustering_values = []
        component_values = []
        
        for window in windows:
            basic_data = self.analysis_results.get(f'{window}_basic', {})
            density_values.append(basic_data.get('density', 0) * 100)
            clustering_values.append(basic_data.get('avg_clustering', 0) * 100)
            component_values.append(basic_data.get('connected_components', 0))
        
        x_pos = np.arange(len(windows))
        width = 0.25
        
        ax2 = ax.twinx()  # Second axe pour les composantes
        
        # DensitÃ© et clustering sur axe principal
        bars1 = ax.bar(x_pos - width/2, density_values, width, 
                      label='DensitÃ© (%)', color='lightblue', alpha=0.7)
        bars2 = ax.bar(x_pos + width/2, clustering_values, width,
                      label='Clustering (%)', color='lightgreen', alpha=0.7)
        
        # Composantes sur second axe
        line = ax2.plot(x_pos, component_values, color='red', marker='o', 
                       linewidth=2, markersize=6, label='Composantes')
        
        ax.set_xlabel('FenÃªtres Temporelles')
        ax.set_ylabel('Pourcentage')
        ax2.set_ylabel('Nombre de Composantes', color='red')
        ax.set_title('MÃ©triques de ConnectivitÃ©', fontweight='bold')
        ax.set_xticks(x_pos)
        ax.set_xticklabels(window_labels)
        
        # LÃ©gendes
        lines1, labels1 = ax.get_legend_handles_labels()
        lines2, labels2 = ax2.get_legend_handles_labels()
        ax.legend(lines1 + lines2, labels1 + labels2, loc='upper left')
        
        ax.grid(True, alpha=0.3)
    
    def _plot_statistical_summary(self, ax):
        """Graphique 10: RÃ©sumÃ© statistique"""
        ax.axis('off')
        
        # Collecter les statistiques clÃ©s
        summary_text = "RÃ‰SUMÃ‰ STATISTIQUE\n" + "="*30 + "\n\n"
        
        for i, (window, (start, end)) in enumerate(self.window_configs.items()):
            basic = self.analysis_results.get(f'{window}_basic', {})
            centralities = self.analysis_results.get(f'{window}_centralities', {})
            
            window_label = f"FenÃªtre {i+1} ({start//60}-{end//60}min)"
            summary_text += f"{window_label}:\n"
            summary_text += f"  â€¢ NÅ“uds: {basic.get('nodes', 0)}\n"
            summary_text += f"  â€¢ ArÃªtes: {basic.get('edges', 0)}\n"
            summary_text += f"  â€¢ DensitÃ©: {basic.get('density', 0)*100:.1f}%\n"
            summary_text += f"  â€¢ Clustering: {basic.get('avg_clustering', 0)*100:.1f}%\n"
            
            if centralities:
                avg_degree = np.mean([data['degree'] for data in centralities.values()])
                max_close_node = max(centralities.items(), key=lambda x: x[1]['closeness'])
                summary_text += f"  â€¢ DegrÃ© moyen: {avg_degree:.1f}\n"
                summary_text += f"  â€¢ Top nÅ“ud: {max_close_node[0]} ({max_close_node[1]['closeness']:.1f}%)\n"
            
            summary_text += "\n"
        
        # Ã‰volution gÃ©nÃ©rale
        summary_text += "Ã‰VOLUTION GÃ‰NÃ‰RALE:\n"
        nodes_evolution = [self.analysis_results.get(f'{w}_basic', {}).get('nodes', 0) 
                          for w in self.window_configs.keys()]
        if all(x > 0 for x in nodes_evolution):
            growth = ((nodes_evolution[-1] - nodes_evolution[0]) / nodes_evolution[0]) * 100
            summary_text += f"  â€¢ Croissance nÅ“uds: {growth:+.1f}%\n"
        
        edges_evolution = [self.analysis_results.get(f'{w}_basic', {}).get('edges', 0) 
                          for w in self.window_configs.keys()]
        if all(x > 0 for x in edges_evolution):
            growth = ((edges_evolution[-1] - edges_evolution[0]) / edges_evolution[0]) * 100
            summary_text += f"  â€¢ Croissance arÃªtes: {growth:+.1f}%\n"
        
        ax.text(0.05, 0.95, summary_text, transform=ax.transAxes, 
               fontsize=9, va='top', ha='left', family='monospace',
               bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))
    
    def export_comprehensive_data(self):
        """Exporter toutes les donnÃ©es d'analyse"""
        print("\nğŸ’¾ Export des donnÃ©es complÃ¨tes...")
        
        # CSV principal avec tous les rÃ©sultats
        output_file = "tme4/ex4/rollernet_comprehensive_analysis.csv"
        
        rows = []
        for window_name in self.window_configs.keys():
            basic = self.analysis_results.get(f'{window_name}_basic', {})
            centralities = self.analysis_results.get(f'{window_name}_centralities', {})
            temporal = self.analysis_results.get(f'{window_name}_temporal', {})
            
            window_label = {'window1': '10-20min', 'window2': '20-30min', 'window3': '30-40min'}[window_name]
            
            # Ligne de rÃ©sumÃ© par fenÃªtre
            summary_row = {
                'window': window_label,
                'type': 'summary',
                'nodes': basic.get('nodes', 0),
                'edges': basic.get('edges', 0),
                'density': basic.get('density', 0),
                'avg_clustering': basic.get('avg_clustering', 0),
                'connected_components': basic.get('connected_components', 0),
                'avg_degree': basic.get('avg_degree', 0),
                'max_degree': basic.get('max_degree', 0)
            }
            rows.append(summary_row)
            
            # DonnÃ©es par nÅ“ud
            if centralities:
                for node, data in centralities.items():
                    node_row = {
                        'window': window_label,
                        'type': 'node',
                        'node_id': node,
                        'degree': data['degree'],
                        'closeness_static': data['closeness'],
                        'betweenness_static': data['betweenness'],
                        'eigenvector': data['eigenvector'],
                        'closeness_temporal': temporal.get(node, 0)
                    }
                    rows.append(node_row)
        
        # Sauvegarder
        df = pd.DataFrame(rows)
        df.to_csv(output_file, index=False)
        print(f"âœ… DonnÃ©es exportÃ©es: {output_file}")
        
        return output_file
    
    def generate_comprehensive_report(self):
        """GÃ©nÃ©rer un rapport complet"""
        print("\nğŸ“„ GÃ©nÃ©ration du rapport complet...")
        
        report_file = "tme4/ex4/rollernet_comprehensive_report.txt"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("TME4 EXERCICE 4 - RAPPORT COMPLET D'ANALYSE ROLLERNET\n")
            f.write("=" * 70 + "\n\n")
            
            f.write("MÃ‰THODOLOGIE INTÃ‰GRÃ‰E:\n")
            f.write("-" * 25 + "\n")
            f.write("â€¢ Exercice 1: Analyse des mÃ©triques de base par fenÃªtre temporelle\n")
            f.write("â€¢ Exercice 2: Calcul des centralitÃ©s statiques (closeness, betweenness)\n")
            f.write("â€¢ Exercice 3: Analyse des chemins temporels avec contrainte 30 minutes\n")
            f.write("â€¢ Integration: Dashboard complet avec visualisations multi-dimensionnelles\n\n")
            
            f.write("CONFIGURATION TEMPORELLE:\n")
            f.write("-" * 25 + "\n")
            for window, (start, end) in self.window_configs.items():
                f.write(f"â€¢ {window}: {start//60}-{end//60} minutes ({start}-{end} secondes)\n")
            f.write("\n")
            
            f.write("RÃ‰SULTATS PAR FENÃŠTRE:\n")
            f.write("-" * 25 + "\n")
            
            for window_name, (start, end) in self.window_configs.items():
                basic = self.analysis_results.get(f'{window_name}_basic', {})
                centralities = self.analysis_results.get(f'{window_name}_centralities', {})
                
                f.write(f"\nFENÃŠTRE {window_name.upper()} ({start//60}-{end//60}min):\n")
                f.write(f"  NÅ“uds: {basic.get('nodes', 0)}\n")
                f.write(f"  ArÃªtes: {basic.get('edges', 0)}\n")
                f.write(f"  DensitÃ©: {basic.get('density', 0)*100:.2f}%\n")
                f.write(f"  Clustering moyen: {basic.get('avg_clustering', 0)*100:.2f}%\n")
                f.write(f"  Composantes connexes: {basic.get('connected_components', 0)}\n")
                
                if centralities:
                    # Top 5 nÅ“uds
                    top_nodes = sorted(centralities.items(), 
                                     key=lambda x: x[1]['closeness'], reverse=True)[:5]
                    f.write(f"  Top 5 nÅ“uds (closeness):\n")
                    for i, (node, data) in enumerate(top_nodes, 1):
                        f.write(f"    {i}. NÅ“ud {node}: {data['closeness']:.1f}% closeness, degrÃ© {data['degree']}\n")
            
            f.write("\nANALYSE COMPARATIVE:\n")
            f.write("-" * 20 + "\n")
            
            # Ã‰volution des mÃ©triques
            nodes_evolution = [self.analysis_results.get(f'{w}_basic', {}).get('nodes', 0) 
                              for w in self.window_configs.keys()]
            edges_evolution = [self.analysis_results.get(f'{w}_basic', {}).get('edges', 0) 
                              for w in self.window_configs.keys()]
            
            f.write(f"â€¢ Ã‰volution nÅ“uds: {nodes_evolution[0]} â†’ {nodes_evolution[1]} â†’ {nodes_evolution[2]}\n")
            f.write(f"â€¢ Ã‰volution arÃªtes: {edges_evolution[0]} â†’ {edges_evolution[1]} â†’ {edges_evolution[2]}\n")
            
            if all(x > 0 for x in nodes_evolution):
                total_growth = ((nodes_evolution[-1] - nodes_evolution[0]) / nodes_evolution[0]) * 100
                f.write(f"â€¢ Croissance totale nÅ“uds: {total_growth:+.1f}%\n")
            
            f.write("\nCONCLUSIONS:\n")
            f.write("-" * 12 + "\n")
            f.write("â€¢ Le rÃ©seau Rollernet montre une Ã©volution dynamique significative\n")
            f.write("â€¢ Les centralitÃ©s varient considÃ©rablement entre fenÃªtres temporelles\n")
            f.write("â€¢ L'analyse temporelle rÃ©vÃ¨le des patterns cachÃ©s vs analyse statique\n")
            f.write("â€¢ Le dashboard intÃ©grÃ© permet une comprÃ©hension holistique du rÃ©seau\n")
            
            f.write(f"\nRAPPORT GÃ‰NÃ‰RÃ‰: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("TME4 Exercice 4 - Analyse ComplÃ¨te RÃ©ussie\n")
        
        print(f"âœ… Rapport gÃ©nÃ©rÃ©: {report_file}")
        return report_file
    
    def run_comprehensive_analysis(self):
        """ExÃ©cuter l'analyse complÃ¨te"""
        print("ğŸš€ DÃ©marrage de l'analyse complÃ¨te...")
        
        # Ã‰tapes d'analyse
        self.load_temporal_data()
        self.build_static_graphs()
        self.analyze_basic_metrics()
        self.analyze_centralities()
        self.analyze_temporal_paths()
        
        # GÃ©nÃ©ration des outputs
        dashboard_file = self.create_comprehensive_dashboard()
        data_file = self.export_comprehensive_data()
        report_file = self.generate_comprehensive_report()
        
        print(f"\nâœ… ANALYSE COMPLÃˆTE TERMINÃ‰E!")
        print(f"ğŸ“Š Dashboard: {dashboard_file}")
        print(f"ğŸ’¾ DonnÃ©es: {data_file}")
        print(f"ğŸ“„ Rapport: {report_file}")
        
        return {
            'dashboard': dashboard_file,
            'data': data_file,
            'report': report_file
        }

def main():
    """Fonction principale TME4 Exercice 4"""
    
    # Fixer les graines pour reproductibilitÃ©
    random.seed(42)
    np.random.seed(42)
    
    try:
        # CrÃ©er l'analyseur
        analyzer = RollernetComprehensiveAnalyzer()
        
        # ExÃ©cuter l'analyse complÃ¨te
        results = analyzer.run_comprehensive_analysis()
        
        print(f"\nğŸ¯ TME4 EXERCICE 4 - SUCCÃˆS COMPLET!")
        print(f"ğŸ”¬ IntÃ©gration rÃ©ussie des mÃ©thodes Ex1+Ex2+Ex3")
        print(f"ğŸ“Š Dashboard multi-dimensionnel gÃ©nÃ©rÃ©")
        print(f"ğŸ’¾ DonnÃ©es exportÃ©es et rapport dÃ©taillÃ© crÃ©Ã©s")
        
    except Exception as e:
        print(f"âŒ Erreur: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 